{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Classification Live Demo\n",
    "\n",
    "Original version by Sean Robertson <https://github.com/spro/practical-pytorch>  \n",
    "Adapted from https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
    "\n",
    "**Goal**: Classify names by language! Examples: Yamaguchi 🇯🇵, Jones 🇬🇧, Asimov 🇷🇺\n",
    "\n",
    "**Plan**: Design a character-level RNN; for tutorial purposes, we won't use `nn.RNN`.\n",
    "\n",
    "**Input**: A tensor of (batch size) x (number of characters in name) x 57  \n",
    "57 is the number of uppercase letters + lowercase letters + permitted symbols `.,;'`\n",
    "\n",
    "**Output**: Tensor of the likelihoods of languages, dimensions (batch size) x (languages)\n",
    "\n",
    "**Caveat**: For simplicity, the RNN we write will not support >1 batch size with variable-sized lengths. Additional code will be required (or just use `nn.RNN` and `pack_padded_sequence` which support >1 batch size with variable-sized lengths)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, pickle\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "names_by_language = pickle.load(open(\"names_by_language.pkl\", \"rb\"))\n",
    "languages = list(names_by_language.keys())\n",
    "num_names = sum(len(names) for names in names_by_language.values())\n",
    "\n",
    "# names_by_language is a dictionary of languages, each with a list of names\n",
    "print(\"Languages:\", languages, end=\"\\n\\n\")\n",
    "print(\"First 10 English names:\", names_by_language[\"English\"][:10], end=\"\\n\\n\")\n",
    "print(num_names, \"names in total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Turning Names into Tensors\n",
    "As mentioned, we want tensors of dimensions (batch size) x (number of characters in name) x 57  \n",
    "Each character will be represented in one-hot form: `[0, ..., 0, 1, 0, ..., 0]`  \n",
    "Note that many programmers/researchers, including the author of the original tutorial, prefer to have the batch size as dimension 1, e.g. (...) x (batch size) x ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, random\n",
    "from itertools import cycle, islice\n",
    "\n",
    "names_array = numpy.zeros((num_names, 99, len(all_letters)), dtype=\"float32\")\n",
    "languages_array = numpy.zeros((num_names), dtype=\"long\")\n",
    "name_index = 0\n",
    "\n",
    "for language_index, language in enumerate(names_by_language.keys()):\n",
    "    for name in names_by_language[language]:\n",
    "        for i, letter in enumerate(name):\n",
    "            try:\n",
    "                names_array[name_index, i, all_letters.find(letter)] = 1\n",
    "            except:\n",
    "                pass\n",
    "        languages_array[name_index] = language_index\n",
    "        name_index += 1\n",
    "\n",
    "print(names_by_language[\"English\"][0])\n",
    "print(names_array[0, 0:6, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's split the array into a train and test set, and convert to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "names_array_train, names_array_test, languages_array_train, languages_array_test = \\\n",
    "    train_test_split(names_array, languages_array, test_size=0.1)\n",
    "\n",
    "names_tensor_train = torch.from_numpy(names_array_train)\n",
    "languages_tensor_train = torch.from_numpy(languages_array_train)\n",
    "names_tensor_test = torch.from_numpy(names_array_test)\n",
    "languages_tensor_test = torch.from_numpy(languages_array_test)\n",
    "\n",
    "print(names_tensor_train.shape)\n",
    "print(languages_tensor_train.shape)\n",
    "print(names_tensor_test.shape)\n",
    "print(languages_tensor_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Datasets and Data Loaders\n",
    "You *don't* have to create datasets and data loaders, but it keeps your code clean and there's no reason not to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "train_dataset = TensorDataset(names_tensor_train, languages_tensor_train)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=1)\n",
    "test_dataset = TensorDataset(names_tensor_test, languages_tensor_test)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create the Network\n",
    "Let's represent our RNN by an object class that subclasses `torch.nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    #fill\n",
    "\n",
    "# Initialize the RNN\n",
    "n_hidden = 128\n",
    "#fill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that our RNN takes as input a **hidden state**, and outputs another **hidden state**.  \n",
    "With PyTorch, **you** define the input(s), output(s), structure, and hyperparameters.  \n",
    "Let's perform a sanity check and make sure the input(s), output(s), and intermediate states are what you expect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed just the 1st letter of the name \"Abbas\"\n",
    "#fill\n",
    "\n",
    "print(\"Input: \", input_tensor.shape)\n",
    "print(\"Hidden:\", hidden_tensor.shape)\n",
    "print(\"Output:\", output_tensor.shape)\n",
    "print(output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4a: Train the Network\n",
    "Our loss function, `NLLLoss`, checks a Negative Log Likelihood (NLL) tensor against a target tensor that specifies which index of the NLL tensor should be highest. Example:  \n",
    "✅ NLL tensor: `[-3, -1, -2]`, target: `[1]`  \n",
    "❌ NLL tensor: `[-1, -3, -2]`, target: `[1]`\n",
    "\n",
    "Let's train for **5 epochs**. We will also be performing basic evaluation *alongside* training. This is a good practice because you can then detect **overfitting**, **insufficient learning rate**, and other issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "train_average_losses = []\n",
    "test_average_losses = []\n",
    "\n",
    "for epoch_index in tqdm(range(5)):\n",
    "\n",
    "    # TRAIN\n",
    "\n",
    "    train_losses = []\n",
    "\n",
    "    for step_index, batch in enumerate(train_dataloader):\n",
    "\n",
    "        #fill\n",
    "\n",
    "    train_average_losses.append(sum(train_losses) / len(train_losses))\n",
    "\n",
    "    # TEST\n",
    "\n",
    "    test_losses = []\n",
    "\n",
    "    #fill\n",
    "\n",
    "    test_average_losses.append(sum(test_losses) / len(test_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_average_losses, \"r\")\n",
    "plt.plot(test_average_losses, \"b\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4b: Add an optimizer\n",
    "You can use an optimizer included with PyTorch rather than writing your own.  \n",
    "While the RNN trains here, we'll write this part in VS Code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the Network\n",
    "There are many metrics for evaluating a classifier, e.g. F1 score, accuracy, etc.  \n",
    "For this tutorial, we'll create a confusion matrix. **Rows are actual/target languages, columns are predictions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "confusion = torch.zeros(len(languages), len(languages))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step_index, batch in enumerate(test_dataloader):\n",
    "            \n",
    "        input_tensor, expected_output = batch\n",
    "        hidden_tensor = rnn.initHidden(input_tensor.shape[0])\n",
    "\n",
    "        for i in range(input_tensor.shape[1]):\n",
    "            if len(torch.nonzero(input_tensor[:, i, :])) > 0:\n",
    "                output_tensor, hidden_tensor = rnn(input_tensor[:, i, :], hidden_tensor)\n",
    "                _, predicted_output = output_tensor.topk(1)\n",
    "        for j in range(output_tensor.shape[0]):\n",
    "            confusion[expected_output[j]][predicted_output[j]] += 1\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + languages, rotation=90)\n",
    "ax.set_yticklabels([''] + languages)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
